{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Supervised Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danilinekamgue/python/blob/master/Supervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8qK1C-lvS7A"
      },
      "source": [
        "# Supervised Learning in Python "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI8CzarvvS7B"
      },
      "source": [
        "If you don't have a proper hardware / software configuration you can use Colab: https://colab.research.google.com/notebooks/intro.ipynb?hl=nb\n",
        "\n",
        "Many libraries are already installed (PyTorch, Tensorflow...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAXWHPu3vS7B"
      },
      "source": [
        "Libraries we will use throughout these tutorials:\n",
        "* numpy\n",
        "* pandas\n",
        "* matplotlib + seaborn\n",
        "* scikit-learn\n",
        "* pytorch + torchvision + tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNelKf4CvS7B"
      },
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icunjQwtvS7C"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxAbFCHovS7C"
      },
      "source": [
        "# Create fake dataset with numbers from 1 to 200\n",
        "x_tosplit = np.arange(1,201,1).reshape(100,2)\n",
        "# Create random targets (0 or 1) for classification problem\n",
        "y_tosplit = np.random.randint(0,2, (x_tosplit.shape[0]))\n",
        "print(x_tosplit.shape)\n",
        "print(y_tosplit.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVXezHElvS7C"
      },
      "source": [
        "# Split dataset with 70% training, 30% test, balancing classes\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_tosplit, y_tosplit, train_size=0.7, shuffle=True, stratify=y_tosplit)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P6TCgXdvS7D"
      },
      "source": [
        "## Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5VNIfxAvS7D"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqYy0VglvS7D"
      },
      "source": [
        "# Read data from file\n",
        "data = pd.read_csv('perceptron-data.csv')\n",
        "labels = torch.tensor(data['target'].values, dtype=torch.float32)\n",
        "data = torch.tensor(data[['x', 'y']].values, dtype=torch.float32)\n",
        "plt.scatter(data[:, 0], data[:, 1], c=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "hVW1iUqivS7D"
      },
      "source": [
        "accuracies = []\n",
        "epochs = 100 # how many times we want to see all dataset\n",
        "eta = 0.1 # learning rate\n",
        "\n",
        "numpt = data.size(0) # number of patterns\n",
        "inputDim = data.size(1)\n",
        "\n",
        "# perceptron is sign(wx+b)\n",
        "weights = torch.randn(inputDim, dtype=torch.float32) # w\n",
        "bias = torch.zeros(1) # b\n",
        "\n",
        "\n",
        "for epoch in range(epochs): # for each epoch\n",
        "    total_accuracy = 0\n",
        "\n",
        "    for idx in range(numpt): # for each pattern in the dataset\n",
        "        X = data[idx,:]\n",
        "        y = labels[idx]\n",
        "\n",
        "        # wx+b\n",
        "        out = torch.add(torch.dot(weights, X), bias).item()\n",
        "        # sign(out)\n",
        "        out = 1 if out > 0 else 0\n",
        "        \n",
        "        # if classification is correct increase accuracy\n",
        "        if out == y:\n",
        "            total_accuracy += 1\n",
        "        \n",
        "        # update weights and bias with perceptron learning rule\n",
        "        weights += eta * (y - out) * X\n",
        "        bias += eta * (y - out)        \n",
        "\n",
        "    accuracies.append(total_accuracy / float(numpt))\n",
        "\n",
        "    \n",
        "# visualize result\n",
        "\n",
        "print(f'Last accuracy: {accuracies[-1]*100}%')\n",
        "# plot points, hyperplane and learning curve\n",
        "plt.figure()\n",
        "plt.scatter(data[:,0].numpy(), data[:,1].numpy(), c=labels.numpy())\n",
        "xr = np.linspace(0, 20, 10)\n",
        "yr = (-1 / weights[1].item()) * (weights[0].item() * xr  + bias.item())\n",
        "plt.plot(xr, yr,'-')\n",
        "plt.xlim(-1, 21)\n",
        "plt.ylim(-3, 22)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(accuracies, '-')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfbNTL-_vS7E"
      },
      "source": [
        "## Multi Layer Perceptron (a.k.a. feedforward networks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hZFFO-fvS7E"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S2ED5WDvS7E"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        :param layers: a list with as many elements as the number of hidden layers.\n",
        "            Each element specifies the number of units in that hidden layer.\n",
        "            e.g. [input_size, 64, 128, output_size]\n",
        "        \"\"\"\n",
        "\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        assert len(layers) >= 2, \"Layers must specify at least input and output size.\"\n",
        "        \n",
        "        # create neural network layers\n",
        "        mlp_layers = []\n",
        "        for i in range(1, len(layers)):\n",
        "            # add a Linear layer \n",
        "            mlp_layers.append(nn.Linear(layers[i-1], layers[i], bias=True))\n",
        "            # add ReLU activation function\n",
        "            mlp_layers.append(nn.ReLU())\n",
        "        \n",
        "        # put all layers together\n",
        "        self.model = nn.Sequential(*mlp_layers)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        This method is called each time you invoke your instance of this class.\n",
        "        e.g. mlp = MLP([10,64,3])\n",
        "            mlp(x) -> will call this method\n",
        "        \"\"\"\n",
        "        \n",
        "        out = self.model(x) # compute output of neural network\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUXhzxbVvS7E"
      },
      "source": [
        "# Read data from file\n",
        "data = pd.read_csv('perceptron-data_notsep.csv')\n",
        "# put data in pytorch tensors\n",
        "labels = torch.tensor(data['target'].values).long() # transform labels from {-1, 1} to {0, 1}\n",
        "data = torch.tensor(data[['x', 'y']].values, dtype=torch.float32)\n",
        "# create a dataloader to automatically shuffle and get batches of data\n",
        "dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True, drop_last=True)\n",
        "numpt = data.size(0)\n",
        "inputDim = data.size(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqKcWdxEvS7F"
      },
      "source": [
        "mlp = MLP([inputDim, 64, 2]) # create neural network\n",
        "\n",
        "# the optimizer is responsible for the updating of network's parameters\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "# the error / cost function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 200 # how many times to loop over the dataset\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "for epoch in range(epochs):\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    for x,y in dataloader: # for each batch of data        \n",
        "        optimizer.zero_grad() # reset gradients, to avoid using previous ones\n",
        "        out = mlp(x) # compute output\n",
        "        loss = criterion(out, y) # compute error\n",
        "        loss.backward() # compute gradients\n",
        "        optimizer.step() # update parameters\n",
        "        \n",
        "        # compute accuracy\n",
        "        with torch.no_grad():\n",
        "            accuracy = (out.argmax(dim=1) == y).sum() / float(y.numel())\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "        \n",
        "    losses.append(total_loss / float(len(dataloader)))\n",
        "    accuracies.append(total_accuracy / float(len(dataloader)))\n",
        "\n",
        "\n",
        "print(f\"Last accuracy: {accuracies[-1]*100}%\")\n",
        "plt.figure()\n",
        "plt.plot(losses, '-')\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.plot(accuracies, '-')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgSUbCK4vS7F"
      },
      "source": [
        "## Some popular metrics (and combinations of them)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQVrtLLJvS7F"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix, \\\n",
        "    precision_recall_curve, plot_precision_recall_curve\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_eGGnRNvS7F"
      },
      "source": [
        "X, y = make_classification(n_classes=2, n_informative=6) # build fake datasset\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True) # split dataset\n",
        "model = LogisticRegression().fit(x_train, y_train) # create a model\n",
        "predictions = model.predict(x_test) # predict on test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WNAPz_avS7G"
      },
      "source": [
        "# F1\n",
        "f1_score(y_test, predictions, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e9HrLHuvS7G"
      },
      "source": [
        "# Confusion matrix\n",
        "confusion_matrix(y_test, predictions, normalize='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "qNvrUl37vS7G"
      },
      "source": [
        "# P-R curve\n",
        "probs = model.decision_function(x_test)\n",
        "p, r, t = precision_recall_curve(y_test, probs)\n",
        "print(p)\n",
        "print(r)\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGSambYQvS7G"
      },
      "source": [
        "plt.plot(r,p)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7Ty4t_xvS7H"
      },
      "source": [
        "plot_precision_recall_curve(model, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbpeMx2-vS7H"
      },
      "source": [
        "# what about ROC curve? Try it out!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAfvvN2qvS7H"
      },
      "source": [
        "## What is tensorboard?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbKanA87vS7H"
      },
      "source": [
        "Why tensorboard? Well... when you cannot debug normally...\n",
        "\n",
        "but also for visualization :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpclRzlXvS7H"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28q4O5WtvS7H"
      },
      "source": [
        "# Writer will output to ./runs/ directory by default\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOrwfOlAvS7H"
      },
      "source": [
        "!ls runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPV-D4DMvS7I"
      },
      "source": [
        "for i in range(20):\n",
        "    writer.add_scalar('loss', i,i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7VdE_WkvS7I"
      },
      "source": [
        "for i in range(20):\n",
        "    writer.add_scalar('loss', i*i,i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWUL03W4vS7I"
      },
      "source": [
        "set x axis to step, otherwise you align results according to execution time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Sx1unBvS7I"
      },
      "source": [
        "for i in range(100):\n",
        "    writer.add_scalars('functions', {'xsinx':i,\n",
        "                                    'xcosx':i**2,\n",
        "                                    'tanx': np.log(i+1)\n",
        "    }, i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}